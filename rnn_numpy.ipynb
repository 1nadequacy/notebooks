{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/comments.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "train_y = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNNumpy(object):\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-1./np.sqrt(word_dim), 1./np.sqrt(word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-1./np.sqrt(hidden_dim), 1./np.sqrt(hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-1./np.sqrt(hidden_dim), 1./np.sqrt(hidden_dim), (word_dim, hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        T = len(x)\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        for t in range(T):\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        loss = 0.0\n",
    "        for i in range(len(y)):\n",
    "            o, s = self.forward(x[i])\n",
    "            for j in range(len(y[i])):\n",
    "                loss += -np.log(o[j, y[i][j]])\n",
    "        return loss\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        total_loss = self.calculate_total_loss(x, y)\n",
    "        N = np.sum(len(y[i]) for i in range(len(y)))\n",
    "        return total_loss / N\n",
    "       \n",
    "    def backward(self, x, y):\n",
    "        T = len(y)\n",
    "        o, s = self.forward(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        \n",
    "        delta_o = o\n",
    "        delta_o[np.arange(T), y] -= 1.\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t]**2))\n",
    "            for tt in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                dLdW += np.outer(delta_t, s[tt-1])\n",
    "                dLdU[:, x[tt]] += delta_t\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[tt-1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        o, s = self.forward(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        \n",
    "        delta_o = o\n",
    "        delta_o[np.arange(T), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t]**2))\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "                #dLdW += np.outer(delta_t, s[bptt_step-1])  \n",
    "                dLdU[:, x[bptt_step]] += delta_t\n",
    "                #dLdU[:,x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]**2)\n",
    "                #delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        gradients = self.bptt(x, y)\n",
    "        params = ['U', 'V', 'W']\n",
    "        for idx, name in enumerate(params):\n",
    "            param = operator.attrgetter(name)(self)\n",
    "            print \"Performing gradient check for parameter %s with size %d.\" % (name, np.prod(param.shape))\n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                i = it.multi_index\n",
    "                value = param[i]\n",
    "                param[i] = value - h\n",
    "                minus = self.calculate_total_loss([x], [y])\n",
    "                param[i] = value + h\n",
    "                plus = self.calculate_total_loss([x], [y])\n",
    "                param[i] = value\n",
    "                approx = (plus - minus) / (2 * h)\n",
    "                grad = gradients[idx][i]\n",
    "                rel_error = np.abs(grad - approx) / (np.abs(grad) + np.abs(approx))  \n",
    "                if i == (0, 0):\n",
    "                    print approx, grad\n",
    "                if rel_error > error_threshold:\n",
    "                    print \"Gradient Check ERROR: parameter=%s ix=%s\" % (name, i)\n",
    "                    print \"+h Loss: %f\" % plus\n",
    "                    print \"-h Loss: %f\" % minus\n",
    "                    print \"Estimated_gradient: %f\" % approx\n",
    "                    print \"Backpropagation gradient: %f\" % grad\n",
    "                    print \"Relative Error: %f\" % rel_error\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print \"Gradient check for parameter %s passed.\" % (name)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "0.0254028668767 0.0254028829584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:96: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "0.00142997246577 0.0014299724647\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "0.0230553273237 0.0230553177573\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 100.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 100.\n",
      "Gradient Check ERROR: parameter=V ix=(0, 0)\n",
      "+h Loss: 7.032527\n",
      "-h Loss: 7.032449\n",
      "Estimated_gradient: 0.039091\n",
      "Backpropagation gradient: -0.178315\n",
      "Relative Error: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:71: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "rnn.gradient_check([1,2, 3], [2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 3513,\n",
       " 7,\n",
       " 155,\n",
       " 794,\n",
       " 25,\n",
       " 223,\n",
       " 8,\n",
       " 32,\n",
       " 20,\n",
       " 202,\n",
       " 5025,\n",
       " 350,\n",
       " 91,\n",
       " 6,\n",
       " 66,\n",
       " 207,\n",
       " 5,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
